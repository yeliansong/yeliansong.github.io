[{"categories":["K8S-study notes"],"content":"K8S study notes","date":"2020-01-25 09:11:51","objectID":"/en/2020-4-14-volume/","tags":["K8S","microservice"],"title":"Volume","uri":"/en/2020-4-14-volume/"},{"categories":["K8S-study notes"],"content":"1. emptyDir It’s the basis of Volume. The life cycle depends on the Pod. The volume will be removed when the related Pod was removed. So we can think of it as the path of the Pod. All the containers of the pod share this voiume. Let’s show how to setup the volume configure. apiVersion: v1 kind: Pod metadata: name: producer-consumer spec: containers: - image: busybox name: producer volumeMounts: - mountPath: /producer_dir name: shared-volume args: - /bin/sh - -c - echo \"hello world\" \u003e /producer_dir/hello; sleep 30000 - image: busybox name: consumer volumeMounts: - mountPath: /consumer_dir name: shared-volume args: - /bin/sh - -c - cat /consumer_dir/hello; sleep 30000 volumes: - name: shared-volume emptyDir: {} The above file is about configure the volume practice. We design two containers in the pod, one is to producer, write the log in the path file, another is to consumer, read the log from the file. It’s easy to understand that these two containers share the storage from the pod. ","date":"2020-01-25 09:11:51","objectID":"/en/2020-4-14-volume/:0:1","tags":["K8S","microservice"],"title":"Volume","uri":"/en/2020-4-14-volume/"},{"categories":["K8S-study notes"],"content":"2. HostPath It’s rare to use this kind of volume. It’s not common. This for Docker host mount the existing path to the Pod. So Pod and containers have very strong coupling. But if pod was removed, the volume is still existing. However, avoid using this kind of volume. ","date":"2020-01-25 09:11:51","objectID":"/en/2020-4-14-volume/:0:2","tags":["K8S","microservice"],"title":"Volume","uri":"/en/2020-4-14-volume/"},{"categories":["K8S-study notes"],"content":"3. Extern Storage Provider This used extern storage provider to achieve the volume. For example, use the AWS，GCE or Azure and so on. Let me show the configure. This used the aws storage. The strength about using this is to make the k8s cluster and volume isolated. There is no impact when the k8s cluster destroyed. ","date":"2020-01-25 09:11:51","objectID":"/en/2020-4-14-volume/:0:3","tags":["K8S","microservice"],"title":"Volume","uri":"/en/2020-4-14-volume/"},{"categories":["K8S-study notes"],"content":"4. PersistentVolume \u0026 PersistentVolumeClaim From section 3, we know we can use the extern storage for the k8s volume. But there is one question, actually the storage system always for administration, but the k8s deployment is for developer. There are different departments. How to balance the gap. The solving method is PV and PVC. PV is maintained by the administration, it’s the extern storage arranged. But PVC is for developer. When deploy the micro service, they can set up the pvc to arrange the space from PV. So PV and PVC coordinate to deploy the whole system. 4.1 NFS PersistentVolume At first, install the NFS service in the master and slave nodes. Also define the mount path. Below are the PV and PVC configure file. nfs-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mypv1 spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /tmp/pv1 server: 10.128.0.2 nfs-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs pod.yaml apiVersion: v1 kind: Pod metadata: name: mypod1 spec: containers: - name: mypod1 image: busybox args: - /bin/sh - -c - sleep 30000 volumeMounts: - mountPath: \"/mydata\" name: mydata volumes: - name: mydata persistentVolumeClaim: claimName: mypvc1 In the PV YAML file, we can define the persistent volume policy type. There are 3 types: Retain,Recycle,and Delete.The default policy is Delete. Retain: Users deletes a persistentVolumeClaim.the corresponding persistentVolume is not deleted. Instead, it’s moved to the release phase, where all of its data can be manually recovered. Recycle: It’s different with Retain, the data can’t be manually recovered when delete the PVC. Delete: It’s default mode. Means a dynamically pv is automatically deleted when a user deleted the corresponding pic. When run the pod, will mount the PVC path. Then use command to write something to the mount path. From the below running result, we can understand the PV and PVC in the NFS. About is the static provision, there is another kind of provision, it’s the dynamical provision. It arranges the storage according to the situation. 4.2 Database Practice Background: use NFS PV and PVC to database backup when the node shutdown to verify the data persistence. Steps: Deploy the PV and PVC according to the below configure files. mysql-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs mysql-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /tmp/mysql-pv server: 10.128.0.2 Deploy the mysql pod and service. piVersion: v1 kind: Service metadata: name: mysql spec: ports: - port: 3306 selector: app: mysql --- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: containers: - image: mysql:5.6 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: password ports: - containerPort: 3306 name: mysql volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: - name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pvc Use the client to access the database, insert some messages into the database. Shutdown the node1, then the database will be automatically backup to the node2. After backup, we can verify in the master node. ","date":"2020-01-25 09:11:51","objectID":"/en/2020-4-14-volume/:0:4","tags":["K8S","microservice"],"title":"Volume","uri":"/en/2020-4-14-volume/"},{"categories":["K8S-study notes"],"content":"K8S study notes","date":"2020-01-20 09:11:51","objectID":"/en/2020-4-11-health-check/","tags":["K8S","microservice"],"title":"Health Check","uri":"/en/2020-4-11-health-check/"},{"categories":["K8S-study notes"],"content":"1. About K8S health check Health check is the important feature of the k8s orchestrating. K8s can monitor those containers and automatically restart them if they fail. If the container’s main process crashes, the k8s will restart the container. Also if your application has a bug that causes it to crash every once in a while, k8s will restart it automatically.There are two kinds of ways for health check. Liveness and Readiness. What’s the difference? And how to use these? liveness probes k8s can check if a container is still alive through liveness probes. You can specify a liveness probe for each container in the pod’s specification. k8s will periodically execute the probe and restart the container if the probe fails. The liveness health check configure file. apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness spec: restartPolicy: OnFailure containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 60 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 From this configure file, we define the livenessProbe. Every 5 seconds, the probe will detect and execute the command. If failed, will restart the pod again. The Readiness health check configure file. apiVersion: v1 kind: Pod metadata: labels: test: readiness name: readiness spec: restartPolicy: OnFailure containers: - name: readiness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 60 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 The file is similar to liveness. Just changed the key value. For this one, when it was failure, will restart the pod one time, after that, set the readiness no use. ","date":"2020-01-20 09:11:51","objectID":"/en/2020-4-11-health-check/:0:1","tags":["K8S","microservice"],"title":"Health Check","uri":"/en/2020-4-11-health-check/"},{"categories":["K8S-study notes"],"content":"2. Health Check practice in the rolling update How the health check used in the rolling update? Image one case, you update the application from V1 to V2, but in fact V2 application is wrong, you didn’t use the checking method to verify this. How will you solve this problem? We can use health check. First, deploy 10 copies with v1. apiVersion: apps/v1 kind: Deployment metadata: name: app spec: replicas: 10 selector: matchLabels: run: app template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 10; touch /tmp/healthy; sleep 30000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 Then rollling update the wrong application. It will trigger the probe to detect. apiVersion: apps/v1 kind: Deployment metadata: name: app spec: replicas: 10 selector: matchLabels: run: app template: metadata: labels: run: app spec: containers: - name: app image: busybox args: - /bin/sh - -c - sleep 3000 readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 10 periodSeconds: 5 ​ When use this configure file to deploy, will fail. Below is the result. From the result, we can see the new replications can’t pass the health check. But the health check also remain the most old replications. This is because of the maxSurge and maxUnavailable. Will use the default value if not define in the file.maxSurge and maxUnavailable are used to define the rolling update copies and failure copies. ","date":"2020-01-20 09:11:51","objectID":"/en/2020-4-11-health-check/:0:2","tags":["K8S","microservice"],"title":"Health Check","uri":"/en/2020-4-11-health-check/"},{"categories":["K8S-study notes"],"content":"3. Rolling update This section is not for health check. Only the summary of the learning before. This section comprises with rolling update and rolling back. Rolling update is very easy, just run the command, will deploy the change automatically. We mainly discuss the rolling back. When you do the rolling update, you can record the revision. When you want to roll back, you can location this version. Kuebctl apply -f rollback.yaml –record When add the record parameter, will record the reversion. Kubectl rollout history deployment http This will show the all deployment history version. Kubectl rollout undo deployment http –to-revision=1 This will rollout the version. Also, when you rollout successful, the version will change as your deployment. ","date":"2020-01-20 09:11:51","objectID":"/en/2020-4-11-health-check/:0:3","tags":["K8S","microservice"],"title":"Health Check","uri":"/en/2020-4-11-health-check/"},{"categories":["K8S-study notes"],"content":"K8S study notes","date":"2020-01-10 10:11:51","objectID":"/en/2020-04-08-how-to-write-the-yaml/","tags":["K8S","microservice"],"title":"How to write YAML","uri":"/en/2020-04-08-how-to-write-the-yaml/"},{"categories":["K8S-study notes"],"content":"1. How to write the YAML apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploymnet spec: replicas: 3 selector: matchLabels: app: web_server template: metadata: labels: app: web_server spec: containers: - name: nginx image: nginx:1.7.9 Okay, Let us analysis this YAML sample. It used the indentation to represent the layer. Every layer was dependent and called each other.The meaning of every key. apiVersion: this is the current configure version. Kind: the resource created type. Metadata: the resource metadata. spec：the deployment’s specification. Selector: use to select which template. Template: define the pod information. So, from this, I think I can describe with mind map.We can refer to below. ","date":"2020-01-10 10:11:51","objectID":"/en/2020-04-08-how-to-write-the-yaml/:0:1","tags":["K8S","microservice"],"title":"How to write YAML","uri":"/en/2020-04-08-how-to-write-the-yaml/"},{"categories":["K8S-study notes"],"content":"2. How to write the job configure file At first, we need to determine the aim of job. The containers can divide into two types, one is for the persistence running for the server, another one is for just one job, one time running. This is the job. How to define Job configure file. apiVersion: batch/v1 kind: Job metadata: name: myjob spec: parallelism: 3 template: metadata: name: myJob spec: containers: - name: hello image: busybox command: [\"echo\", \"hello k8s job\" ] restartPolicy: OnFailure Actually there are many kinds of job configures, including regular job, parallelism job and timing job. Also need to flexible use the restartPolicy parameter. OnFailure means, when failed, will restart the pod. You can set “never” as well means will restart the pod until it ’s successful. ","date":"2020-01-10 10:11:51","objectID":"/en/2020-04-08-how-to-write-the-yaml/:0:2","tags":["K8S","microservice"],"title":"How to write YAML","uri":"/en/2020-04-08-how-to-write-the-yaml/"},{"categories":["K8S-study notes"],"content":"3. Summary of YAML In the end, I think YAML is not difficult if you master the rules. It’s comprised with layers. So if you understand every layer, you can design your deployment, job or services. ","date":"2020-01-10 10:11:51","objectID":"/en/2020-04-08-how-to-write-the-yaml/:0:3","tags":["K8S","microservice"],"title":"How to write YAML","uri":"/en/2020-04-08-how-to-write-the-yaml/"},{"categories":["K8S-study notes"],"content":"K8S study notes","date":"2020-01-04 09:11:51","objectID":"/en/2020-3-22-understand-the-k8s-architecture/","tags":["K8S","microservice"],"title":"Understand The K8S Architecture","uri":"/en/2020-3-22-understand-the-k8s-architecture/"},{"categories":["K8S-study notes"],"content":"1. k8s components architecture. Okay, it’s very easy to understand. Like above, master node is the most important. It controls the other nodes to join the cluster. Also API server is exposed outside. Through the master node, then can arrange the other nodes. ","date":"2020-01-04 09:11:51","objectID":"/en/2020-3-22-understand-the-k8s-architecture/:0:1","tags":["K8S","microservice"],"title":"Understand The K8S Architecture","uri":"/en/2020-3-22-understand-the-k8s-architecture/"},{"categories":["K8S-study notes"],"content":"2. Practice Target: Create the k8s cluster, then deploy the application to these 2 nodes. Steps. Use kubeadm init to create the master k8s cluster, then join node1 and node2 to the cluster. In the master node, deploy the application with replications as 2. After execute that, deployment is completed. You can execute command “kubectl get pods”, show the deployment. The application is deployed separately in the 2 nodes. ","date":"2020-01-04 09:11:51","objectID":"/en/2020-3-22-understand-the-k8s-architecture/:0:2","tags":["K8S","microservice"],"title":"Understand The K8S Architecture","uri":"/en/2020-3-22-understand-the-k8s-architecture/"},{"categories":["K8S-study notes"],"content":"3. Understand the process This is the whole process. When you execute “kubectl run”, will call the API Server, then API Server notice the Controller Server to create the deployment resource. After that, Schedule service will arrange the tasks to the node1 and node2. In the node1 and node2, kubelet will create the pods according to the tasks. Kube-proxy will arrange the network config for these 2 nodes. ","date":"2020-01-04 09:11:51","objectID":"/en/2020-3-22-understand-the-k8s-architecture/:0:3","tags":["K8S","microservice"],"title":"Understand The K8S Architecture","uri":"/en/2020-3-22-understand-the-k8s-architecture/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"1. Several ways to upgrade Let’s look at a case first, how to deal with this situation: The POD is currently using the V1 version, and now there is a V2 version, how to replace the V2 version with the V1 version? There are three ways: Delete the old version POD and replace it with the new version POD This method is more violent. It is to directly modify the configuration template of V1 to V2, and then delete the POD of V1. At this time, the template of V2 will be detected. If there is no POD, the POD of V2 version will be restarted. One problem with this approach is that the entire service stops when V1 is deleted. blue-green deployment This is like this, that is, your program supports V1 and V2 versions at the same time, and your environment resources are sufficient to run these two at the same time. How to do it? It is to start the V2 POD in your environment. After everything is ok, you can delete the V1 POD and switch the service traffic to V2. The problem with this method is that the resource overhead is relatively large. rolling upgrade This is more powerful. It uses capacity expansion and contraction to realize dynamic replacement and upgrade. What do you mean, is to define the old and new versions of PODs in your deployment file, and then delete the V1 PODs in turn to dynamically expand the V2 PODs. But in this way, if you use commands to operate, it is more cumbersome. ","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/:0:1","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"2. Use ReplicationController to realize automatic rolling upgrade Use the kubectl command to perform rolling upgrades. kubectl rolling-update kubia-v1 kubia-v2 –image=luksa/kubia:v2 command. What exactly does this command do? Let’s see. You can look at this picture, which is to gradually replace the POD of V1 with V2 by modifying the number of Replicas. But there is a problem with this method, because the Kubectl method is used to complete these, which is equivalent to the client method, so if there is no network, the upgrade may stop. ","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/:0:2","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"3. Use Deployment to upgrade the application Deployment is a more advanced upgrade method. It is an upgrade method belonging to the upper layer, while the other types belong to the bottom layer. When using Deployment, ReplicaSet is actually used to manage POD. So how do you do rolling upgrades? Deployment is a deployment file, in which you can define the information you want to deploy, and then deploy. A deployment is defined, now let’s see how to implement a rolling upgrade. First, the upgrade needs to be triggered. How to trigger it? It is triggered by modifying the Deployment file. You can use Kubectl set image deployment kubia nodejs=luksa/kubia:v2 to re-specify the image, and the POD will download the image again. Then internally, it will delete the POD of V1 in turn, and then start the POD of V2. It can also define its own upgrade strategy, RollinUpdate and Recreate, the default is the first. ","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/:0:3","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"4. Rollback upgrade Now there is a problem, if you want to upgrade a V3 version, but there are some bugs in the V3 version, you upgrade, and then find that there is a problem, how to go back to the previous version? It can be solved by command: kubectl rollout undo deployment kubia, you can roll back to the previous version. You can also roll back to a certain historical version. You can view the history first: kubectl rollout history deployment kubia to view all historical versions, and then use kubectl rollout undo deployment kubia –to-revision = ? to roll back to a certain version. ","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/:0:4","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"5. State control of rolling upgrade Control upgrade rate It is defined with two parameters: maxSurge and maxUnavailable. I don’t really want to figure out how to calculate it. You can look at the effect. Pause rolling upgrade This can be used to play canary upgrades. What is a canary is bound. How to play, is to do a rolling upgrade first, and then immediately suspend the rolling upgrade. At this time, a new POD will be created, the old POD is still running, and some traffic will be switched to the new POD, and then test, test After passing, you can resume the rolling upgrade, complete the replacement, and if the test fails, you can roll back to the previous version. The canary upgrade is actually to let some people experience the new version. Take a look at the command: kubectl rollout pause deployment kubia kubectl rollout resume deployment kubia Prevent rolling upgrades of buggy versions There is another better way, which is to set a time period and set the probe. During this time period, if there is no problem with the new POD, rolling upgrade is supported. This is equivalent to setting up an insurance that can minimize the risk. The deadline configured for rolling upgrades It is to set a time period. If the program is not upgraded successfully, the upgrade can be canceled. ","date":"2019-11-01 15:11:51","objectID":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/:0:5","tags":["K8S","microservice"],"title":"九、Deployment declarative upgrade application","uri":"/en/2019-09-16-9-deployment%E5%A3%B0%E6%98%8E%E5%BC%8F%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-10-12 15:11:51","objectID":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/","tags":["K8S","microservice"],"title":"八、Access POD metadata and other resources from applications","uri":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/"},{"categories":["K8S-study notes"],"content":"1. Downward API passing metadata To be honest, this chapter is also in the clouds, and it feels similar to the environment variable configuration. My understanding is that the Downward API is used to configure the metadata of the POD or container, and the environment variables are used to define some regular variable parameters. Downward API supports environment variables and files, which is more flexible. ","date":"2019-10-12 15:11:51","objectID":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/:0:1","tags":["K8S","microservice"],"title":"八、Access POD metadata and other resources from applications","uri":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/"},{"categories":["K8S-study notes"],"content":"2. Kubernetes API server interaction In fact, this section mainly talks about how to use the Kubernetes API. Using the API is actually accessing its server. Each level of directory is a first-level resource. This is to access the API server through the ambassador container. Of course, the API server can also be accessed through the client. The content of this chapter is mainly to configure the metadata of POD and access resources through API. ","date":"2019-10-12 15:11:51","objectID":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/:0:2","tags":["K8S","microservice"],"title":"八、Access POD metadata and other resources from applications","uri":"/en/2019-09-16-8-%E4%BB%8E%E5%BA%94%E7%94%A8%E8%AE%BF%E9%97%AEpod%E5%85%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-10-10 14:11:51","objectID":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/","tags":["K8S","microservice"],"title":"七、ConfigMap and Secret configuration application","uri":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"},{"categories":["K8S-study notes"],"content":"1. Configuration of container command line parameters Docker uses command line parameters by defining Entrypoint and CMD in Dockerfile, but there is a problem, this is packaged into the image. Let’s take a look at the difference between the two first. Entrypoint: Defines the executable program that is invoked when the container starts. CMD: Specifies the parameters passed to Entrypoint. So the usual practice is to define these two. When starting with the command line, you can use the argu of the command line to override the definition of CMD in the file. Of course, there is another way, which is the script method, which can write commands into the script, and then define and call the script in the dockerfile. Commands defined in the POD can override the container’s command line. So arguments can be passed to the container via argu in the POD’s YAML file. ","date":"2019-10-10 14:11:51","objectID":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/:0:1","tags":["K8S","microservice"],"title":"七、ConfigMap and Secret configuration application","uri":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"},{"categories":["K8S-study notes"],"content":"2. General operation of POD environment variables Before introducing ConfigMap, let’s take a look at the general operation of POD configuration environment variables. In fact, you have to understand that environment variables are for containers, so one way of thinking is this. Define environment variables in the container, and the programs in the container reference the environment variables, and then assign values to the environment variables in the POD configuration file. , which is normal operation. Just like this, an environment variable is defined, and then, this variable is assigned in the POD. How about this method, okay? Obviously it is not good. Without decoupling, the environment variables and POD configuration files are strongly related. So is there a good way to achieve decoupling? That is, this configuration file can be used not only here, but also in other places . This uses ConfigMap. ","date":"2019-10-10 14:11:51","objectID":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/:0:2","tags":["K8S","microservice"],"title":"七、ConfigMap and Secret configuration application","uri":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"},{"categories":["K8S-study notes"],"content":"3. ConfigMaps First of all, we must understand the problem it needs to solve. In fact, it is equivalent to an independent configuration file, and then it can be referenced in different environments. So this thing is pretty good. Creating a ConfigMap can be done through commands or YAML files. How to create it is not described in detail. This example is to use POD to rewrite the environment variable INTERVAL in the container. The basic process is this: Don’t explain it, it’s clear. But there is a question, what if this configuration file does not exist, can this POD still be up, the answer is no. It will wait until the configuration file exists before the container can start. Of course, you can also set parameters so that it can start POD without a configuration file. Of course, if your config file contains a lot of key-values, it is troublesome to set them one by one, but it can be done in one step. prefix is used to set the prefix, and there is another one. The key-value set in the configuration file must be in accordance with the format, otherwise, Kubernetes cannot parse it. ","date":"2019-10-10 14:11:51","objectID":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/:0:3","tags":["K8S","microservice"],"title":"七、ConfigMap and Secret configuration application","uri":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"},{"categories":["K8S-study notes"],"content":"4. secert configuration file Of course, the configuration files cannot be all named texts, and those that need to be encrypted need to be configured in secert mode. The secret is also a key-value method like configMap, but there are still some differences. The secret exists in the memory of the POD, so when the POD is deleted, there is no such configuration. ","date":"2019-10-10 14:11:51","objectID":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/:0:4","tags":["K8S","microservice"],"title":"七、ConfigMap and Secret configuration application","uri":"/en/2019-09-13-7-configmap-%E5%92%8C-secret%E9%85%8D%E7%BD%AE%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-10-03 14:11:51","objectID":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/","tags":["K8S","microservice"],"title":"六、Volume - mount the disk to the container","uri":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"1. volume Finally waiting for you, volume, volume. It is also a very important component of kubernetes. No, it should not be said to be a component of k8s, it should be said to be a part of POD. What problem is the volume used to solve? POD is actually equivalent to a logical host, and each POD may have multiple containers. We know that these containers are actually processes equivalent to logical hosts. These processes can share CPU, RAM, network, etc., but each container is With their own file systems, these systems are isolated from each other. Is there a way for these containers to share the file system? You think about this scenario, for example, a container in the POD is hung up, and then a new one is started, but this one still uses the file system resources of the previous one. How to solve this problem, do you have to use volumes, which is equivalent to using shared ones? this file system. Look at this, there are three containers running in one POD, each of which is an independent file system, do you think this can run? Let me introduce these three containers, webserver, which is the entrance of the web, content is used to store web page information, and log is used to store logs. These three do not have a shared file system. When accessing, the web container cannot find the web page, nor can the log be stored. . Come on, check out the improved version. After sharing the volume, hahaha, it’s great to be able to run. ","date":"2019-10-03 14:11:51","objectID":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/:0:1","tags":["K8S","microservice"],"title":"六、Volume - mount the disk to the container","uri":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"2. Several types of volumes A volume is actually a storage medium, and it uses the storage medium of the host, that is, the POD host. Therefore, the performance of the volume is affected by the logical host. Of course, you can also set the volume to use memory storage. According to usage habits, there are many types of volumes. Introduce two kinds: emptyDir It is also the simplest type of volume, and it is also the basis of other volumes. It is easy to understand that it is an empty volume. Its life cycle is consistent with the life cycle of POD. This is when the POD starts, a blank space is initialized to store the content. This is an example, each image can set the path of the volume, and then the type of the volume can be set on the volume node. This can define whether to use memory or storage media. Git storage volume This volume is also very interesting, it is designed for git warehouse, This is the model diagram. When the POD starts, it will pull the remote git warehouse into this volume, but this cannot be synchronized in real time. I understand that it will not be so advanced. The prototype of the GIT volume is an empty volume, so the life cycle is consistent with that of the POD. ![1568261085518]( HostPath Volume Can files be shared between PODs? Yes, this is the HostPath volume. Look at this picture, two PODs share this HostPath volume, so even if you delete a POD, it will not affect this volume. But there is a problem, this volume is strongly related to POD, that is, if you use other POD, you cannot access this volume. persistent volume This part is compatible with very different environments. If you are using GCP, there are corresponding component support, and other cloud platforms also have other corresponding components. So I won’t talk about this part. To figure out its usage scenario, this requires data persistence, that is, data access can be performed across different nodes. Follow-up is useful enough to focus on. ","date":"2019-10-03 14:11:51","objectID":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/:0:2","tags":["K8S","microservice"],"title":"六、Volume - mount the disk to the container","uri":"/en/2019-09-12-6-%E5%8D%B7-%E5%B0%86%E7%A3%81%E7%9B%98%E6%8C%82%E8%BD%BD%E5%88%B0%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"1. Service and port The service is actually easy to understand. Think about it, how do you manage a group of the same POD? Do you need to know all their IPs, and then manually configure and connect them one by one? Obviously not needed, how to solve this problem, use services, services are actually equivalent to a routing function. Look at this, there are 3 PODs on the front end, and a Backend, how to make the whole system run normally, using services. The three front-ends set up a front-end service to expose an IP, and the Backend does the same to expose a service, so each part of the connection only focuses on the service IP, regardless of the network information of each POD. This is the YAML for creating a service, KIND is a Service, and then maps the port of the POD, as well as a label selector. Services can be created with kubectl expose or kubectl create. ![1568172425531]( Is this fun? It is to test whether the POD can be accessed through the service. It is to exec to a POD first, and then access the ip of this service through curl to see if it can connect to other PODs. It’s very simple. Let’s talk about ports. A POD can have one port or multiple ports, which is easy to understand. ","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/:0:1","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"2. Service Discovery In fact, it is to use environment variables to connect to this service instead of using a fixed IP. After all, the IP may change. You can select a POD, then kubectl exec kubia-3inly env to list the environment variables, then view the environment variables of this service, and provide other client connections through variables. ","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/:0:2","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"3. Endpoint This guy is more interesting. Look at the picture. It’s very clear. It’s equivalent to mapping the service to two Endpoints. Can it be understood as load balancing? It’s a bit like that. ","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/:0:3","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"4. The method of exposing the service to external clients Remember, three methods. NodePort I don’t know the meaning of this setting. What does it mean? It is equivalent to reserving a PORT for all nodes, and then you have two ways to access it. The first is clusterIp: port, the second is nodeIp: port has three port numbers, the first is the port of the service cluster, The second is the target port of the POD, and the third is the nodePort port. As you can see from this picture, 30123 is the port of nodeport, through this port Add the IP of node to access the node. load balancing method It uses an external IP from the cloud infrastructure, which is a unique and publicly accessible IP address, through which it is redirected to the service. There is nothing else to say about other feelings, just watch this. ![1568194400337](This loads A balancer can average traffic and see what its policy is. ingress A picture to see its function: ![1568195843695](What do you mean Well, that is to say, you can selectively access services through domain names. ![1568195964547](This The picture shows how to access POD through ingress. ","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/:0:4","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"5. Readiness Probe Another fucking amazing thing. It really needs to deal with various situations. The readiness probe is used to deal with this kind of situation. That is, when you start a POD, it is impossible to get it up immediately, but kube doesn’t know it. It may still give you the traffic. , So how to avoid this problem, use the ready pointer, that is to say, set a pointer to notify kube of success or failure. ![1568196997031]( A ready pointer is set, which is to execute the ls command in the container to see if the file exists. ","date":"2019-10-01 11:11:51","objectID":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/:0:5","tags":["K8S","microservice"],"title":"五、POD service and communication","uri":"/en/2019-09-11-5-pod%E7%9A%84%E6%9C%8D%E5%8A%A1%E4%B8%8E%E9%80%9A%E4%BF%A1/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-09-17 11:11:51","objectID":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/","tags":["K8S","microservice"],"title":"四、Deploy managed POD by copy mechanism","uri":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/"},{"categories":["K8S-study notes"],"content":"1. Survival Probe Why come up with such a profound word? We know that if a container program in the POD crashes, kubenetes will restart the program, but the problem is, if it is not a crash, such as a deadlock, or throwing an exception, etc., what to do at this time, kubenetes does not know Whether your container is normal or not, you need to use a probe at this time. In fact, the reason is very simple, that is, use a probe to detect it from time to time to see if it is normal. Three kinds of probes: 1) HTTP GET probe, which is used to check whether the network is normal. 2) The TCP socket probe is similar, it is to establish a TCP connection, if it succeeds, it is normal. 3) The Exec probe is to execute any command in the container, and if it returns normally, it is successful. A probe is actually a POD, and some attributes can be defined to control some operations of the probe. ","date":"2019-09-17 11:11:51","objectID":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/:0:1","tags":["K8S","microservice"],"title":"四、Deploy managed POD by copy mechanism","uri":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/"},{"categories":["K8S-study notes"],"content":"2. ReplicationController It is also a very powerful thing, that is, copy control. It is used to ensure the number of PODs and can be used for horizontal expansion of PODs. RC has three parts: label selector, label selector, used to determine which PODs are in the RC scope; RC COUNT: used to specify the number of PODs; POD TEMP: template, to create a new copy of POD. Let’s take a look at the impact of tags on RC. In fact, it is easy to understand that RC is only responsible for the tags defined in it, and monitors the number of PODs of these tags at all times. When the POD of these tags decreases or increases, it will dynamically adjust. So, there are two situations, the first one, if you modify the label in the running POD, it means that the POD used to belong to A, and then it belongs to B later. At this time, RC has to do something, and he will recreate the POD . In the second case, if you modify the label of RC, then all the PODs in this label will be out of monitoring, and all of them will be free. Play around with kubectl edit rc kubia, and you will find that you can modify the current RC, and then modifying the current RC will have no effect on the currently running POD, but will only affect the subsequent POD. Interesting, three PODs, after deleting the RC, these three PODs will not be affected, and they will be freed from the shackles of the RC. But, don’t use RC, use ReplicaSet, this buddy has the same function as RC, but the function has been enhanced a lot, mainly because the use of tags is more flexible, and the others are exactly the same as RC, so don’t stress about this. ","date":"2019-09-17 11:11:51","objectID":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/:0:2","tags":["K8S","microservice"],"title":"四、Deploy managed POD by copy mechanism","uri":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/"},{"categories":["K8S-study notes"],"content":"3. DaemonSet What is this used for? It is used to control the deployment of POD, which means to deploy POD according to its own definition. Come, come, look at this product. The ReplicaSet is used on the left. The deployed PODs are disorganized on each node. Now I require only one POD to be deployed on each node. How to deal with this situation? This will use DaemonSet. This is the YAML file of DaemonSet, nodeSelector node scheduler. ","date":"2019-09-17 11:11:51","objectID":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/:0:3","tags":["K8S","microservice"],"title":"四、Deploy managed POD by copy mechanism","uri":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/"},{"categories":["K8S-study notes"],"content":"4. JOB This is used to define a task, which is to end after completion. Rather than hoping to run it all the time, this is also a good use. JOB also has a lot of application settings, you can set the delay time of the task and so on. ","date":"2019-09-17 11:11:51","objectID":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/:0:4","tags":["K8S","microservice"],"title":"四、Deploy managed POD by copy mechanism","uri":"/en/2019-09-11-4-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E9%83%A8%E7%BD%B2%E6%89%98%E7%AE%A1pod/"},{"categories":["K8S-study notes"],"content":"Reading Notes for K8S Learning","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"1. Why create a POD We know that POD is the smallest unit managed by Kubernetes. Why does Kubernetes not directly manage containers, but manage PODs? There is a reason for this. We know that a container is a single process. What does it mean? The design idea of a container is that each container only runs one process. If you use dry bernetes to directly manage these containers, it must be complicated, so I will do it The POD is introduced to realize resource and network isolation between containers. All containers under the same POD share network and system resources. However, all containers in a POD must be a whole that is closely integrated with logical services. They There must be a strong connection. Each container can be distinguished by port. ","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/:0:1","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"2. How to plan the containers in the POD As I just said, a group of containers are placed in a POD. How to plan these containers? Is it possible to put all the containers in the project into a POD? Because POD is relatively lightweight, it is encouraged to use as many PODs as possible. When planning the containers in POD, you need to consider whether the business logic of your group of containers is closely connected, and whether you want to scale your containers. , these are your judgment conditions. A good example is that the front-end and back-end are divided into two PODs for deployment. ","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/:0:2","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"3. YAML file analysis and POD creation Very EASY, in fact, a YAML file contains: kind, metadata, spe these things. metadata is used to define the data of POD, and spec is used to define all container data under this POD. There are several commands to learn: Kubectl create -f a.yaml, this is to create the required POD through the yaml file. To obtain the log information of the specified container in the POD, you can use kubectl logs kula -c kubia. To obtain the yaml or json format file of the running POD, you can use kubectl get po kubia -o yaml or kubectl get po kubia -o json. kubectl port-forward kubela 8888:8080 is used for port mapping. ","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/:0:3","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"4. POD label There is no doubt that the role of labels is to distinguish PODs. So adding tags is to better manage your POD. A POD can set multiple tags, and then use the tags to schedule PODs uniformly. Some commands of the label can be viewed through the manual, including modifying labels, setting labels, adding labels, and so on. ","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/:0:4","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-study notes"],"content":"5. Namespace Let’s figure out first, the purpose of creating a Namespace. We know that we already have the concept of labels, and labels can also be used for grouping, but why do we still need to create a Namespace? The grouping of tags is still incomplete. A POD can have multiple tags, so I think tags are still used more by O\u0026M personnel. The Namespace is different. Each POD has only one Namespace. Grouping is more thorough. Let’s see how to play Namespace. kubectl get ns: List all namespaces, kubectl get po –namespace kube-system: List all PODs of the specified Namespace. kubectl create namespace custom-namespace, create a Namespace. ","date":"2019-09-15 11:16:51","objectID":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/:0:5","tags":["K8S","microservice"],"title":"三、Containers running in Kubernetes","uri":"/en/2019-09-10-3-%E8%BF%90%E8%A1%8C%E4%BA%8Ekubernetes%E4%B8%AD%E7%9A%84%E5%AE%B9%E5%99%A8/"},{"categories":["K8S-Study Notes"],"content":"Reading Notes for K8S Learning","date":"2019-09-11 19:16:51","objectID":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":["K8S","microservice"],"title":"二、The use of Kubernetes and Docker","uri":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["K8S-Study Notes"],"content":"1. Understanding Docker images, file systems and containers The image of the container is a file after packaging and compiling the container. The packaging image depends on the Dockerfile file. The From line defines the starting content of the image, which is the basic image for construction. The process of building the image is to upload the entire directory of files to the Docker guardian During the process, Docker will first pull the basic image from the basic image warehouse, and then the packaging of the image is a layered structure, with the basic image as a layer, and then each command will be used as a new layer, layer by layer , the whole is a joint file system. The understanding of the container, the container is actually an instance of the image running. The container runs on the operating system of the host, so there will be a problem, such as an image packaged on the RedHat host, can this image run on ubuntu? Not necessarily, it depends on whether you use something specific to the host, but you are running something that is not available on another host. The container running from the image is equivalent to a process on the host machine. The file system between each container is also independent. ","date":"2019-09-11 19:16:51","objectID":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/:0:1","tags":["K8S","microservice"],"title":"二、The use of Kubernetes and Docker","uri":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["K8S-Study Notes"],"content":"2. Kubernetes cluster and Mini Kube Let’s first understand the concept of a cluster. What is a cluster? A cluster is a combination of multiple computer nodes to form a cluster, so the advantages of the cluster are also obvious, which can better manage the system and provide the performance of a stand-alone system. Take a look at mini kube, it is a local single-node cluster, it is not suitable for multi-node situations. To interact with kubernetes, you also need the kubernetes cli client. Overview of multi-node Kubernetes clusters: A kubernetes diagram of three nodes. Take a look at this thing, each working node has Docker, Kubelet and Kube-proxy, and you can use kubectl to send control commands to the master node through rest to control each sub-node. interesting. ","date":"2019-09-11 19:16:51","objectID":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/:0:2","tags":["K8S","microservice"],"title":"二、The use of Kubernetes and Docker","uri":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["K8S-Study Notes"],"content":"3. ReplicationController POD is the smallest unit controlled by Kubernetes. Kubernetes does not care about container scheduling, it only manages POD. There can be many containers in a POD, but this group of containers is closely related because they can be regarded as running on the same host. So what is ReplicationControler, and what is it used for. According to my understanding, ReplicationControler is used to manage the horizontal expansion of POD, which can ensure that the specified number of POD can operate normally according to the setting. You can look at this picture, after external access, it will only be mapped to an internal ip, and then we don’t care which POD is called, because each POD inside is an independent ip. ReplicationControler to control the number of copies. ","date":"2019-09-11 19:16:51","objectID":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/:0:3","tags":["K8S","microservice"],"title":"二、The use of Kubernetes and Docker","uri":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["K8S-Study Notes"],"content":"4. Sort out some related commands of kubectl kubectl get pods, list all PODs, kubectl expose kubia –type=LoadBalancer –name kubia-http. Set the service network to LoadBalancer mode, kubectl get service, list all services kubectl get replicationcontrollers, lists all replicas. kubectl scale rc kubia –replicas=3, expand the replica. kubectl get rc, check capacity expansion. kubectl get pods -o wide, to view the ip of the POD and the running nodes. kubectl describe pod, you can view the log information of this POD. ","date":"2019-09-11 19:16:51","objectID":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/:0:4","tags":["K8S","microservice"],"title":"二、The use of Kubernetes and Docker","uri":"/en/2019-09-10-2-kubernetes-%E5%92%8Cdocker%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["K8S-Study Notes"],"content":"Reading Notes for K8S Learning","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"1. Comparison between monolithic applications and microservices Disadvantages of monolithic applications: run on several servers in the form of a single process or several processes, the deployment cycle is long, and the operation and maintenance and development are disconnected. After the developers complete the development, they are packaged into a whole for operation and maintenance deployment. There is no decoupling between modules. To modify a module, the whole package needs to be deployed. The advantages of microservices: decoupling between modules, shortening the deployment cycle, and a single module can achieve independent development, deployment, upgrade, and scaling. Reduce deployment time. In addition to saving resources, modules can be separated Scale up all of them, not all of them. ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:1","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"2. DevOps Microservices brought about DevOps. The previous team combination was that after the developers completed the development, they gave the results to the operation and maintenance, and then the operation and maintenance completed the deployment, and the development and operation and maintenance were completely separated. This brings about a problem. Developers cannot understand the needs of users at the first time, and they do not understand the operating environment of the system, and the operation and maintenance do not understand the functions of the product. After the advent of microservices, through the virtualization of system hardware resources by kubernetes, developers do not have to worry about the deployment environment, simplifying system deployment, developers can also participate in the deployment, and DevOps is derived. ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:2","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"3. Container technology Container technology is actually the isolation technology of Linux. Isolation by what? Namespaces and CGROUPs. Namespaces are used to isolate different processes, and CGROUPs are used to isolate resources. It is through this that a host can run multiple different applications. Two applications on the same host can also share files. If they share the same basic image, they share resources. However, files at this layer are read-only. If they are overwritten and written, they are written on the basis of the basic image. A layer of files, the container is a joint file system. ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:3","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"4. Porting of containers The image of the container is a complete system, which has been trimmed. What it cuts is still the host machine, so whether a container image can run or not depends on the environment in which the image is running. If the kernel version of the host environment where the image is orchestrated is consistent with the operating environment, it can run. At the same time, it also depends on whether some system resources unique to the host are used. ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:4","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"5. Kubernetes Google came up with this thing to solve the management deployment problem of Google’s tens of thousands of servers. After the container technology became popular, Kubernetes supported containers, so it can be said that Kubernetes has made Docker. Let’s be consistent. The design purpose of Kubernetes is to abstract the underlying infrastructure and provide you with a master node to manage thousands of running nodes of your service. You don’t have to care about the deployment of your service running nodes. ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:5","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"categories":["K8S-Study Notes"],"content":"6. The composition of Kubernetes Master node: API server (function entry, management of other modules), Schedule (scheduling application), Control Manager, etcd (distributed data storage) Working node: kubelet (communicating with API, managing containers), kube-proxy (responsible for network traffic balancing) ","date":"2019-09-10 21:56:51","objectID":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/:0:6","tags":["K8S","microservice"],"title":"一、Introduction to Kubernetes","uri":"/en/2019-09-10-1%E4%B8%80kubernetes%E7%9A%84%E4%BB%8B%E7%BB%8D/"}]